"""
æ£€ç´¢å™¨è¯„ä¼°ç³»ç»Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯T2Rankingæ•°æ®é›†åŠ è½½å’Œè¯„ä¼°åŠŸèƒ½
"""

import asyncio
from app.services.dataset_loader import DatasetService
from app.services.retriever_evaluation import RetrieverEvaluator


async def test_t2ranking_loader():
    """æµ‹è¯•T2Rankingæ•°æ®é›†åŠ è½½"""
    print("=" * 60)
    print("æµ‹è¯• T2Ranking æ•°æ®é›†åŠ è½½")
    print("=" * 60)
    
    # æ•°æ®é›†è·¯å¾„
    collection_path = "/Users/yeruijian/Documents/çŸ¥è¯†åº“å¹³å°/dataset/T2Ranking/data/collection.tsv"
    queries_path = "/Users/yeruijian/Documents/çŸ¥è¯†åº“å¹³å°/dataset/T2Ranking/data/queries.dev.tsv"
    qrels_path = "/Users/yeruijian/Documents/çŸ¥è¯†åº“å¹³å°/dataset/T2Ranking/data/qrels.dev.tsv"
    
    try:
        # åŠ è½½æ•°æ®é›†ï¼ˆé‡‡æ ·ï¼‰
        dataset = DatasetService.load_t2ranking(
            collection_path=collection_path,
            queries_path=queries_path,
            qrels_path=qrels_path,
            max_queries=10,  # åªåŠ è½½10ä¸ªæŸ¥è¯¢ç”¨äºæµ‹è¯•
            max_docs=1000    # æœ€å¤š1000ä¸ªæ–‡æ¡£
        )
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = dataset.get_statistics()
        print("\næ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # è·å–æµ‹è¯•ç”¨ä¾‹
        test_cases = dataset.get_test_cases(limit=5)
        print(f"\nç”Ÿæˆ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆç¤ºä¾‹ï¼‰:")
        for i, case in enumerate(test_cases[:3], 1):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i}:")
            print(f"  Query ID: {case['query_id']}")
            print(f"  Query: {case['query'][:100]}...")
            print(f"  ç›¸å…³æ–‡æ¡£æ•°: {len(case['relevant_doc_ids'])}")
            print(f"  ç›¸å…³æ–‡æ¡£ID: {case['relevant_doc_ids'][:5]}...")
        
        print("\nâœ… T2Rankingæ•°æ®é›†åŠ è½½æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except FileNotFoundError as e:
        print(f"\nâŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return False
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_retriever_evaluator():
    """æµ‹è¯•æ£€ç´¢å™¨è¯„ä¼°åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ£€ç´¢å™¨è¯„ä¼°åŠŸèƒ½")
    print("=" * 60)
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = RetrieverEvaluator(top_k=10)
        
        # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
        retrieved_doc_ids = ["doc_1", "doc_2", "doc_5", "doc_10", "doc_15"]
        relevant_doc_ids = ["doc_2", "doc_5", "doc_7", "doc_20"]
        
        print("\næ¨¡æ‹Ÿæ£€ç´¢åœºæ™¯:")
        print(f"  æ£€ç´¢åˆ°çš„æ–‡æ¡£: {retrieved_doc_ids}")
        print(f"  ç›¸å…³çš„æ–‡æ¡£: {relevant_doc_ids}")
        
        # è¯„ä¼°å•ä¸ªæŸ¥è¯¢
        metrics = evaluator.evaluate_single_query(
            retrieved_doc_ids=retrieved_doc_ids,
            relevant_doc_ids=relevant_doc_ids
        )
        
        print("\nè¯„ä¼°æŒ‡æ ‡:")
        for metric_name, value in metrics.items():
            print(f"  {metric_name}: {value:.4f}")
        
        # æ‰¹é‡è¯„ä¼°
        batch_results = [
            {
                'retrieved_doc_ids': ["doc_1", "doc_2", "doc_3"],
                'relevant_doc_ids': ["doc_2", "doc_3", "doc_4"]
            },
            {
                'retrieved_doc_ids': ["doc_5", "doc_6", "doc_7"],
                'relevant_doc_ids': ["doc_7", "doc_8"]
            },
            {
                'retrieved_doc_ids': ["doc_10", "doc_11"],
                'relevant_doc_ids': ["doc_10", "doc_11", "doc_12"]
            }
        ]
        
        avg_metrics = evaluator.evaluate_batch(batch_results)
        
        print("\næ‰¹é‡è¯„ä¼°å¹³å‡æŒ‡æ ‡:")
        for metric_name, value in avg_metrics.items():
            print(f"  {metric_name}: {value:.4f}")
        
        print("\nâœ… æ£€ç´¢å™¨è¯„ä¼°åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "ğŸš€" * 30)
    print("æ£€ç´¢å™¨è¯„ä¼°ç³»ç»Ÿæµ‹è¯•")
    print("ğŸš€" * 30 + "\n")
    
    # æµ‹è¯•æ•°æ®é›†åŠ è½½
    result1 = await test_t2ranking_loader()
    
    # æµ‹è¯•è¯„ä¼°åŠŸèƒ½
    result2 = await test_retriever_evaluator()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"T2Rankingæ•°æ®é›†åŠ è½½: {'âœ… é€šè¿‡' if result1 else 'âŒ å¤±è´¥'}")
    print(f"æ£€ç´¢å™¨è¯„ä¼°åŠŸèƒ½: {'âœ… é€šè¿‡' if result2 else 'âŒ å¤±è´¥'}")
    
    if result1 and result2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ£€ç´¢å™¨è¯„ä¼°ç³»ç»Ÿå·²å°±ç»ªã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    asyncio.run(main())


"""
T2Rankingæ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•å¯¼å…¥å’Œä½¿ç”¨T2Rankingæ•°æ®é›†è¿›è¡Œæ£€ç´¢å™¨è¯„ä¼°
"""

import asyncio
from app.services.dataset_loader import DatasetService
from app.services.retriever_evaluation import RetrieverEvaluator, RetrievalTestRunner


# é…ç½®T2Rankingæ•°æ®é›†è·¯å¾„
COLLECTION_PATH = "/Users/yeruijian/Documents/çŸ¥è¯†åº“å¹³å°/dataset/T2Ranking/data/collection.tsv"
QUERIES_PATH = "/Users/yeruijian/Documents/çŸ¥è¯†åº“å¹³å°/dataset/T2Ranking/data/queries.dev.tsv"
QRELS_PATH = "/Users/yeruijian/Documents/çŸ¥è¯†åº“å¹³å°/dataset/T2Ranking/data/qrels.dev.tsv"


async def example_1_load_dataset():
    """ç¤ºä¾‹1: åŠ è½½T2Rankingæ•°æ®é›†"""
    print("=" * 60)
    print("ç¤ºä¾‹1: åŠ è½½T2Rankingæ•°æ®é›†")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®é›†ï¼ˆå»ºè®®é‡‡æ ·ä»¥ä¼˜åŒ–æ€§èƒ½ï¼‰
    dataset = DatasetService.load_t2ranking(
        collection_path=COLLECTION_PATH,
        queries_path=QUERIES_PATH,
        qrels_path=QRELS_PATH,
        max_queries=100,  # é‡‡æ ·100ä¸ªæŸ¥è¯¢
        max_docs=None     # è‡ªåŠ¨æ ¹æ®æŸ¥è¯¢ç¡®å®šç›¸å…³æ–‡æ¡£
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = dataset.get_statistics()
    print("\næ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"  æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
    print(f"  å¹³å‡æ¯ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£æ•°: {stats['avg_relevant_docs_per_query']:.2f}")
    
    # è·å–æµ‹è¯•ç”¨ä¾‹
    test_cases = dataset.get_test_cases(limit=5)
    print(f"\nç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"ç¤ºä¾‹æŸ¥è¯¢: {test_cases[0]['query'][:50]}...")
    
    return dataset


async def example_2_evaluate_retriever():
    """ç¤ºä¾‹2: è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹2: è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½")
    print("=" * 60)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RetrieverEvaluator(top_k=10)
    
    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœï¼ˆå®é™…ä½¿ç”¨æ—¶ï¼Œè¿™äº›æ˜¯ä»å‘é‡æ•°æ®åº“æ£€ç´¢çš„ç»“æœï¼‰
    retrieved_docs = ["doc_1", "doc_2", "doc_5", "doc_10", "doc_15"]
    relevant_docs = ["doc_2", "doc_5", "doc_7", "doc_20"]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = evaluator.evaluate_single_query(retrieved_docs, relevant_docs)
    
    print("\nè¯„ä¼°æŒ‡æ ‡:")
    print(f"  Precision@10: {metrics['precision']:.4f}")
    print(f"  Recall@10: {metrics['recall']:.4f}")
    print(f"  F1-Score: {metrics['f1_score']:.4f}")
    print(f"  MRR: {metrics['mrr']:.4f}")
    print(f"  MAP: {metrics['map']:.4f}")
    print(f"  NDCG: {metrics['ndcg']:.4f}")
    print(f"  Hit Rate: {metrics['hit_rate']:.4f}")


async def example_3_batch_evaluation():
    """ç¤ºä¾‹3: æ‰¹é‡è¯„ä¼°å¤šä¸ªæŸ¥è¯¢"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹3: æ‰¹é‡è¯„ä¼°å¤šä¸ªæŸ¥è¯¢")
    print("=" * 60)
    
    evaluator = RetrieverEvaluator(top_k=10)
    
    # æ¨¡æ‹Ÿå¤šä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
    batch_results = [
        {
            'retrieved_doc_ids': ["doc_1", "doc_2", "doc_3", "doc_4"],
            'relevant_doc_ids': ["doc_2", "doc_3", "doc_5"]
        },
        {
            'retrieved_doc_ids': ["doc_10", "doc_11", "doc_12"],
            'relevant_doc_ids': ["doc_11", "doc_12", "doc_13"]
        },
        {
            'retrieved_doc_ids': ["doc_20", "doc_21"],
            'relevant_doc_ids': ["doc_20", "doc_22", "doc_23"]
        }
    ]
    
    # æ‰¹é‡è¯„ä¼°
    avg_metrics = evaluator.evaluate_batch(batch_results)
    
    print(f"\n{len(batch_results)} ä¸ªæŸ¥è¯¢çš„å¹³å‡æ€§èƒ½:")
    print(f"  å¹³å‡ Precision: {avg_metrics['precision']:.4f}")
    print(f"  å¹³å‡ Recall: {avg_metrics['recall']:.4f}")
    print(f"  å¹³å‡ F1-Score: {avg_metrics['f1_score']:.4f}")
    print(f"  å¹³å‡ MRR: {avg_metrics['mrr']:.4f}")


async def example_4_compare_retrievers():
    """ç¤ºä¾‹4: å¯¹æ¯”ä¸åŒæ£€ç´¢å™¨é…ç½®"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹4: å¯¹æ¯”ä¸åŒæ£€ç´¢å™¨é…ç½®")
    print("=" * 60)
    
    evaluator = RetrieverEvaluator(top_k=10)
    
    # ç›¸åŒæŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£
    relevant_docs = ["doc_2", "doc_5", "doc_7", "doc_10"]
    
    # é…ç½®Açš„æ£€ç´¢ç»“æœï¼ˆå¦‚ BM25ï¼‰
    config_a_results = ["doc_1", "doc_2", "doc_5", "doc_8", "doc_9"]
    metrics_a = evaluator.evaluate_single_query(config_a_results, relevant_docs)
    
    # é…ç½®Bçš„æ£€ç´¢ç»“æœï¼ˆå¦‚ Dense Retrievalï¼‰
    config_b_results = ["doc_2", "doc_5", "doc_7", "doc_10", "doc_12"]
    metrics_b = evaluator.evaluate_single_query(config_b_results, relevant_docs)
    
    print("\né…ç½®å¯¹æ¯”:")
    print(f"é…ç½®A (BM25):")
    print(f"  F1-Score: {metrics_a['f1_score']:.4f}")
    print(f"  NDCG: {metrics_a['ndcg']:.4f}")
    
    print(f"\né…ç½®B (Dense Retrieval):")
    print(f"  F1-Score: {metrics_b['f1_score']:.4f}")
    print(f"  NDCG: {metrics_b['ndcg']:.4f}")
    
    # åˆ¤æ–­å“ªä¸ªé…ç½®æ›´å¥½
    winner = "é…ç½®B" if metrics_b['f1_score'] > metrics_a['f1_score'] else "é…ç½®A"
    print(f"\nç»“è®º: {winner} æ€§èƒ½æ›´ä¼˜")


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "ğŸ“š" * 30)
    print("T2Rankingæ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹")
    print("ğŸ“š" * 30 + "\n")
    
    # ç¤ºä¾‹1: åŠ è½½æ•°æ®é›†
    await example_1_load_dataset()
    
    # ç¤ºä¾‹2: è¯„ä¼°æ£€ç´¢å™¨
    await example_2_evaluate_retriever()
    
    # ç¤ºä¾‹3: æ‰¹é‡è¯„ä¼°
    await example_3_batch_evaluation()
    
    # ç¤ºä¾‹4: å¯¹æ¯”é…ç½®
    await example_4_compare_retrievers()
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ æç¤º:")
    print("  - ä½¿ç”¨ max_queries å‚æ•°æ§åˆ¶æ•°æ®é›†è§„æ¨¡")
    print("  - å»ºè®®ä»100ä¸ªæŸ¥è¯¢å¼€å§‹æµ‹è¯•")
    print("  - è¯„ä¼°æŒ‡æ ‡è¶Šæ¥è¿‘1.0è¶Šå¥½")
    print("  - F1-Scoreå¹³è¡¡äº†ç²¾ç¡®ç‡å’Œå¬å›ç‡")
    print("  - NDCGè€ƒè™‘äº†æ£€ç´¢ç»“æœçš„æ’åºè´¨é‡")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    asyncio.run(main())

